"""Vertex AIå®¢æˆ·ç«¯"""

import asyncio
import json
import time
import uuid
import re
import httpx
from typing import Dict, Any, Optional, List, AsyncGenerator

from src.core import TokenStatsManager, CredentialManager, MODELS_CONFIG_FILE
from src.stream import get_stream_processor, AuthError as StreamAuthError
from src.utils import autocorrect_diff
from src.utils.image import extract_images_from_assistant_message

# ä»æ‹†åˆ†çš„æ¨¡å—å¯¼å…¥
from .chunk_aggregator import ChunkAggregator
from .message_builder import MessageBuilder
from .model_config import ModelConfigBuilder


class AuthError(Exception):
    """è®¤è¯é”™è¯¯"""
    pass


class VertexAIClient:
    """Vertex AI APIå®¢æˆ·ç«¯"""
    
    def __init__(self, cred_manager: CredentialManager, stats_manager: TokenStatsManager,
                 request_token_refresh_callback=None):
        self.cred_manager = cred_manager
        self.stats_manager = stats_manager
        self.request_token_refresh = request_token_refresh_callback
        
        # ä¼˜åŒ–è¿æ¥æ± é…ç½®ï¼Œæå‡å…¼å®¹æ€§
        limits = httpx.Limits(
            max_keepalive_connections=20,
            max_connections=100,
            keepalive_expiry=30.0  # æ˜¾å¼è®¾ç½® keepalive è¿‡æœŸæ—¶é—´
        )
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(connect=30.0, read=120.0, write=30.0, pool=10.0),
            limits=limits,
            http1=True,   # å¯ç”¨ HTTP/1.1 æ”¯æŒ
            http2=True,   # åŒæ—¶å¯ç”¨ HTTP/2 æ”¯æŒ
        )
    
    def _create_isolated_client(self) -> httpx.AsyncClient:
        """
        ä¸ºæµå¼è¯·æ±‚åˆ›å»ºéš”ç¦»çš„httpxå®¢æˆ·ç«¯
        
        å¢å¼ºå…¼å®¹æ€§é…ç½®ï¼š
        - åŒæ—¶æ”¯æŒ HTTP/1.1 å’Œ HTTP/2
        - æ›´å®½æ¾çš„è¶…æ—¶è®¾ç½®
        - ä¼˜åŒ–è¿æ¥æ± ç®¡ç†
        """
        limits = httpx.Limits(
            max_keepalive_connections=5,  # å¢åŠ  keepalive è¿æ¥æ•°
            max_connections=10,            # å¢åŠ æœ€å¤§è¿æ¥æ•°
            keepalive_expiry=60.0          # å»¶é•¿ keepalive è¿‡æœŸæ—¶é—´
        )
        
        return httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=30.0,   # å¢åŠ è¿æ¥è¶…æ—¶
                read=180.0,     # å¢åŠ è¯»å–è¶…æ—¶ï¼ˆé•¿å“åº”ï¼‰
                write=30.0,     # å¢åŠ å†™å…¥è¶…æ—¶
                pool=30.0       # å¢åŠ æ± ç­‰å¾…è¶…æ—¶
            ),
            limits=limits,
            follow_redirects=True,  # å¯ç”¨é‡å®šå‘è·Ÿéš
            http1=True,             # å¯ç”¨ HTTP/1.1ï¼ˆæŸäº›æœåŠ¡å™¨ä¸æ”¯æŒ HTTP/2ï¼‰
            http2=True,             # åŒæ—¶å¯ç”¨ HTTP/2
            verify=True
        )

    async def complete_chat(self, messages: List[Dict[str, str]], model: str, **kwargs) -> Dict[str, Any]:
        """èšåˆæµå¼å“åº”ä¸ºéæµå¼ChatCompletionå¯¹è±¡"""
        
        full_content = ""
        reasoning_content = ""
        finish_reason = "stop"
        
        _raw_image_response = kwargs.pop('_raw_image_response', False)

        async for chunk_data_sse in self.stream_chat(messages, model, **kwargs):
            if chunk_data_sse.startswith("data: "):
                json_str = chunk_data_sse[6:].strip()
                if json_str == "[DONE]":
                    continue
                
                try:
                    chunk = json.loads(json_str)
                    choices = chunk.get('choices', [])
                    if choices:
                        delta = choices[0].get('delta', {})
                        
                        if 'content' in delta:
                            full_content += delta['content']
                        if 'reasoning_content' in delta:
                            reasoning_content += delta['reasoning_content']
                        if choices[0].get('finish_reason'):
                            finish_reason = choices[0]['finish_reason']
                            
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSON è§£æé”™è¯¯: {e}")
                    
        full_content = autocorrect_diff(full_content)

        if full_content.startswith("![Generated Image](data:"):
            print("â„¹ï¸ æ£€æµ‹åˆ°å›¾åƒå“åº” (éæµå¼)")
            data_url = full_content[21:-1]
            
            if _raw_image_response:
                try:
                    header, encoded = data_url.split(',', 1)
                    return {
                        "created": int(time.time()),
                        "data": [{"b64_json": encoded}]
                    }
                except Exception as e:
                    print(f"âŒ è§£æå›¾åƒ URL å¤±è´¥: {e}")
                    return {"created": int(time.time()), "data": []}
            else:
                return {"resultUrl": data_url}
            
        if '<tool_calls>' in full_content and '</tool_calls>' in full_content:
            print("â„¹ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å—")
            final_content = full_content
            response = {
                "id": f"chatcmpl-proxy-nonstream-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "usage": self.stats_manager.get_current_usage(),
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": final_content
                        },
                        "finish_reason": "stop" # Finish reason is 'stop' as it's a text response
                    }
                ]
            }
            return response
        
        final_content = full_content
        if reasoning_content:
            cleaned_reasoning = re.sub(r'\n\s*\n', '\n', reasoning_content).strip()
            final_content = f"**Reasoning:**\n{cleaned_reasoning}\n\n**Response:**\n{full_content}"
        
        if not final_content:
            final_content = " "
            
        response = {
            "id": f"chatcmpl-proxy-nonstream-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "usage": self.stats_manager.get_current_usage(),
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_content
                    },
                    "finish_reason": finish_reason
                }
            ]
        }
        return response

    async def stream_chat(self, messages: List[Dict[str, str]], model: str, **kwargs):
        """æµå¼èŠå¤© - ä¼˜åŒ–ç‰ˆï¼ˆæ”¯æŒè¯·æ±‚é˜Ÿåˆ—å’Œé¢„åˆ·æ–°ï¼‰"""
        request_id = str(uuid.uuid4())[:8]  # ç”Ÿæˆè¯·æ±‚IDç”¨äºè¿½è¸ª
        if not self.cred_manager.latest_harvest or (time.time() - self.cred_manager.last_updated > 3000):
            async with self.cred_manager.refresh_lock:
                should_refresh = False
                if not self.cred_manager.latest_harvest:
                    should_refresh = True
                elif time.time() - self.cred_manager.last_updated > 3000:
                    print("âš ï¸ å‡­è¯å·²è¿‡æœŸ (>50åˆ†é’Ÿ)ï¼Œè§¦å‘åˆ·æ–°...")
                    should_refresh = True
                
                if should_refresh:
                    if self.request_token_refresh:
                        await self.request_token_refresh()
                    
                    print(f"[{request_id}] â³ ç­‰å¾…æ–°å‡­è¯...")
                    # ä½¿ç”¨æ–°çš„é˜Ÿåˆ—æœºåˆ¶ç­‰å¾…
                    refreshed = await self.cred_manager.wait_for_credential_with_queue(request_id, timeout=60)
                    
                    if refreshed:
                        await asyncio.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿç¡®ä¿å‡­è¯å°±ç»ª
                    
                    if not refreshed and not self.cred_manager.latest_harvest:
                        error_msg = "âš ï¸ **Proxy Error**: Could not refresh credentials.\n\nPlease ensure **Google Vertex AI Studio** is open in your browser and the Harvester script is active."
                        chunk = {
                            "id": "error-no-creds",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": "vertex-ai-proxy",
                            "choices": [{"index": 0, "delta": {"content": error_msg}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        yield "data: [DONE]\n\n"
                        return

        # é¢„åˆ·æ–°æ£€æµ‹ï¼šå¦‚æœå‡­è¯å³å°†è¿‡æœŸï¼Œæå‰è§¦å‘åˆ·æ–°
        if self.cred_manager.should_preemptive_refresh(threshold=120):
            print(f"[{request_id}] ğŸ”„ å‡­è¯å³å°†è¿‡æœŸï¼Œè§¦å‘é¢„åˆ·æ–°...")
            if self.request_token_refresh:
                # å¼‚æ­¥è§¦å‘åˆ·æ–°ï¼Œä¸é˜»å¡å½“å‰è¯·æ±‚
                asyncio.create_task(self.request_token_refresh())

        max_retries = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
        content_yielded = False
        isolated_client = self._create_isolated_client()
        
        try:
            for attempt in range(max_retries + 1):
                stream_processor = get_stream_processor()
                stream_processor.enable_debug(True)
                
                # è®°å½•å½“å‰å‡­è¯ç‰ˆæœ¬
                current_cred_version = self.cred_manager.credential_version
                
                creds = self.cred_manager.get_credentials()
                if not creds:
                    if attempt > 0:
                        break
                    return

                raw_body = creds['body']
                if isinstance(raw_body, dict):
                    original_body = raw_body
                else:
                    original_body = json.loads(raw_body)
            
                system_instruction = ""
                chat_history = []
                all_assistant_images_with_turn = []
                
                last_user_index = -1
                assistant_turn_number = 0
                for i, msg in enumerate(messages):
                    if msg['role'] == 'user':
                        last_user_index = i
                    elif msg['role'] == 'assistant':
                        assistant_turn_number += 1
                        assistant_content = msg['content'] if isinstance(msg['content'], str) else ""
                        if assistant_content and 'data:image/' in assistant_content and ';base64,' in assistant_content:
                            _, image_parts = extract_images_from_assistant_message(assistant_content)
                            if image_parts:
                                for img_part in image_parts:
                                    all_assistant_images_with_turn.append((assistant_turn_number, img_part))
                
                if all_assistant_images_with_turn:
                    print(f"â„¹ï¸ å…±æ”¶é›† {len(all_assistant_images_with_turn)} å¼ å†å²å›¾ç‰‡")
                
                for i, msg in enumerate(messages):
                    if msg['role'] == 'system':
                        # å¤„ç† system æ¶ˆæ¯çš„ content å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
                        if isinstance(msg['content'], str):
                            system_instruction += msg['content'] + "\n"
                        elif isinstance(msg['content'], list):
                            # å¦‚æœæ˜¯åˆ—è¡¨,æå–æ‰€æœ‰æ–‡æœ¬éƒ¨åˆ†
                            for part in msg['content']:
                                if isinstance(part, dict) and part.get('type') == 'text':
                                    system_instruction += part.get('text', '') + "\n"
                                elif isinstance(part, str):
                                    system_instruction += part + "\n"
                    elif msg['role'] == 'user':
                        parts = []
                        
                        if i == last_user_index and all_assistant_images_with_turn:
                            parts.append({"text": f"[ä»¥ä¸‹æ˜¯ä¹‹å‰ç”Ÿæˆçš„ {len(all_assistant_images_with_turn)} å¼ å›¾ç‰‡ï¼š]"})
                            current_turn = 0
                            for turn_num, img_part in all_assistant_images_with_turn:
                                if turn_num != current_turn:
                                    current_turn = turn_num
                                    parts.append({"text": f"[ç¬¬ {turn_num} è½®ç”Ÿæˆçš„å›¾ç‰‡:]"})
                                parts.append(img_part)
                            
                            parts.append({"text": "[ä»¥ä¸Šæ˜¯å†å²å›¾ç‰‡ï¼Œç”¨æˆ·æ–°è¯·æ±‚å¦‚ä¸‹:]"})
                            print(f"â„¹ï¸ æ³¨å…¥ {len(all_assistant_images_with_turn)} å¼ å†å²å›¾ç‰‡")
                        
                        if isinstance(msg['content'], str):
                            parts.append({"text": msg['content']})
                        elif isinstance(msg['content'], list):
                            for part in msg['content']:
                                if part['type'] == 'text':
                                    parts.append({"text": part['text']})
                                elif part['type'] == 'image_url':
                                    image_url = part['image_url']['url']
                                    if image_url.startswith('data:'):
                                        header, encoded = image_url.split(',', 1)
                                        mime_type = header.split(':')[1].split(';')[0]
                                        parts.append({
                                            "inlineData": {
                                                "mimeType": mime_type,
                                                "data": encoded
                                            }
                                        })
                        chat_history.append({"role": "user", "parts": parts})
                    elif msg['role'] == 'assistant':
                        assistant_content = msg['content'] if isinstance(msg['content'], str) else ""
                        
                        if assistant_content and 'data:image/' in assistant_content and ';base64,' in assistant_content:
                            cleaned_text, _ = extract_images_from_assistant_message(assistant_content)
                            
                            if cleaned_text.strip():
                                chat_history.append({"role": "model", "parts": [{"text": cleaned_text}]})
                            else:
                                # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œæ·»åŠ ä¸€ä¸ªç®€çŸ­è¯´æ˜
                                chat_history.append({"role": "model", "parts": [{"text": "[å·²ç”Ÿæˆå›¾ç‰‡]"}]})
                        else:
                            # æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œç›´æ¥æ·»åŠ 
                            if assistant_content:
                                chat_history.append({"role": "model", "parts": [{"text": assistant_content}]})

                # 2. Construct New Body
                # We clone the harvested body structure to keep all the magic context/metadata
                new_variables = original_body.get('variables', {}).copy()
                
                # Update contents (Chat History)
                new_variables['contents'] = chat_history
                
                # Inject Tools into System Instruction (Custom Format)
                if 'tools' in kwargs and kwargs['tools']:
                    print(f"â„¹ï¸ æ³¨å…¥ {len(kwargs['tools'])} ä¸ªå·¥å…·åˆ°ç³»ç»Ÿæç¤º")
                    tools_xml = "\n\n<available_tools>\n"
                    for tool in kwargs['tools']:
                        function = tool.get('function', {})
                        tools_xml += f"  <tool>\n"
                        tools_xml += f"    <name>{function.get('name', '')}</name>\n"
                        tools_xml += f"    <description>{function.get('description', '')}</description>\n"
                        # Ensure parameters are serialized to a string
                        params = function.get('parameters', {})
                        tools_xml += f"    <parameters>{json.dumps(params)}</parameters>\n"
                        tools_xml += f"  </tool>\n"
                    tools_xml += "</available_tools>\n"
                    
                    # Add instruction for the model to use the specific XML format expected by the parser
                    tools_xml += "\nIMPORTANT: To use a tool, you MUST output a <tool_calls> block. "
                    system_instruction += tools_xml

                # Update System Instruction
                if system_instruction:
                    new_variables['systemInstruction'] = {"parts": [{"text": system_instruction.strip()}]}

                # Disable Safety Filters
                new_variables['safetySettings'] = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_CIVIC_INTEGRITY", "threshold": "BLOCK_NONE"}
                ]

                # CLEANUP: Remove tools and toolConfig to prevent context interference
                # Harvester might capture a session with tools enabled (e.g. Google Search),
                # which can confuse the model if we don't intend to use them.
                # new_variables.pop('tools', None)
                # new_variables.pop('toolConfig', None)
                    
                # Update Model
                # Load model mapping from models.json
                model_map = {}
                try:
                    with open(MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        model_map = config.get('alias_map', {})
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ models.json å¤±è´¥: {e}")

                target_model = model_map.get(model, model)
                
                # Handle suffixes for thinking and resolution
                thinking_mode = None
                resolution_mode = None
                
                if target_model.endswith("-low"):
                    target_model = target_model[:-4]
                    thinking_mode = "low"
                elif target_model.endswith("-high"):
                    target_model = target_model[:-5]
                    thinking_mode = "high"
                    
                if target_model.endswith("-1k"):
                    resolution_mode = "1k"
                    target_model = target_model[:-3]
                elif target_model.endswith("-2k"):
                    resolution_mode = "2k"
                    target_model = target_model[:-3]
                elif target_model.endswith("-4k"):
                    resolution_mode = "4k"
                    target_model = target_model[:-3]

                # The target_model variable already holds the base model name (stripped of resolution suffix)
                # if a resolution suffix was present. We use it directly as the backend model ID.
                backend_model_for_api = target_model
                
                # ç®€åŒ–æ¨¡å‹åˆ‡æ¢æ—¥å¿—
                new_variables['model'] = backend_model_for_api
                
                # Apply generation parameters from client
                if 'generationConfig' not in new_variables:
                    new_variables['generationConfig'] = {}
                
                gen_config = new_variables['generationConfig']

                # Handle Thinking Config
                # Case 1: Explicit suffixes (-low, -high)
                if thinking_mode:
                    gen_config['thinkingConfig'] = {"includeThoughts": True}
                    if thinking_mode == 'low':
                         budget = 8192
                    elif thinking_mode == 'high':
                         budget = 32768
                    
                    gen_config['thinkingConfig']['budget_token_count'] = budget
                    gen_config['thinkingConfig']['thinkingBudget'] = budget
                    print(f"â„¹ï¸ æ€è€ƒæ¨¡å¼: {thinking_mode}, é¢„ç®—: {budget}")

                # Case 2: No suffix, but client provided max_tokens (treat as thinking budget for 3-pro)
                # Only applies if we haven't already set a thinking mode via suffix
                elif 'gemini-3-pro' in target_model and 'max_tokens' in kwargs and kwargs['max_tokens'] is not None:
                    budget = int(kwargs['max_tokens'])
                    # Only enable thinking if budget is reasonable for thinking (e.g. > 1024)
                    # or if user explicitly wants it. Let's assume max_tokens on 3-pro implies thinking budget.
                    gen_config['thinkingConfig'] = {
                        "includeThoughts": True,
                        "budget_token_count": budget,
                        "thinkingBudget": budget
                    }
                    print(f"â„¹ï¸ æ€è€ƒæ¨¡å¼ (è‡ªå®šä¹‰): é¢„ç®—={budget}")
                
                # Handle Resolution (Image Generation)
                # New logic: Check for "image" in model name, then check for resolution suffix.
                if "image" in target_model:
                    # This is an image model. Ensure response modalities are set.
                    if 'responseModalities' not in gen_config:
                        gen_config['responseModalities'] = ["TEXT", "IMAGE"]
                    if 'imageConfig' not in gen_config:
                        gen_config['imageConfig'] = {}
                    
                    # Set other standard image generation parameters from logs
                    gen_config['imageConfig']['personGeneration'] = "ALLOW_ALL"
                    if 'imageOutputOptions' not in gen_config['imageConfig']:
                        gen_config['imageConfig']['imageOutputOptions'] = {"mimeType": "image/png"}

                    # Only add imageSize if a resolution suffix is present
                    if resolution_mode:
                        size_str_map = {
                            "1k": "1K",
                            "2k": "2K",
                            "4k": "4K"
                        }
                        if resolution_mode in size_str_map:
                            gen_config['imageConfig']['imageSize'] = size_str_map[resolution_mode]
                            print(f"â„¹ï¸ å›¾åƒç”Ÿæˆ: å°ºå¯¸={gen_config['imageConfig']['imageSize']}")
                    else:
                        # If no suffix, remove any existing imageSize to let Google decide
                        gen_config['imageConfig'].pop('imageSize', None)
                        print(f"â„¹ï¸ å›¾åƒç”Ÿæˆ: é»˜è®¤å°ºå¯¸")
                
                # CLEANUP: Remove model-specific configurations that might cause conflicts
                # If we switch models, old generation configs (like thinking) might be invalid.
                
                # Remove 'thinkingConfig' if present, unless the model is explicitly a thinking model
                if not thinking_mode:
                    gen_config.pop('thinkingConfig', None)
                    # Also check for snake_case just in case
                    gen_config.pop('thinking_config', None)

                # Remove 'imageConfig' if NOT an image model (to be safe)
                # ä¿®å¤: ä½¿ç”¨ target_model åˆ¤æ–­æ˜¯å¦ä¸ºå›¾åƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯ resolution_mode
                # å› ä¸º resolution_mode åªåœ¨æœ‰ -1k/-2k/-4k åç¼€æ—¶æ‰è®¾ç½®
                if "image" not in target_model:
                    gen_config.pop('imageConfig', None)
                    gen_config.pop('sampleImageSize', None)
                    gen_config.pop('width', None)
                    gen_config.pop('height', None)
                    # æ¸…ç† responseModalities - éå›¾åƒæ¨¡å‹ä¸åº”è¯¥æœ‰å¤šæ¨¡æ€è¾“å‡ºé…ç½®
                    # å¦åˆ™ä¼šå¯¼è‡´ "Multi-modal output is not supported" é”™è¯¯
                    gen_config.pop('responseModalities', None)
                
                # Note: The exact field name might be 'thinkingConfig' or inside 'generationConfig'
                # Based on common Vertex AI payloads, let's check 'generationConfig'
                
                # Fix maxOutputTokens
                # Allow client to override max_tokens, otherwise default to harvested value or 65535
                # client_max_tokens = original_body.get('variables', {}).get('generationConfig', {}).get('maxOutputTokens')
                
                # Check if client provided max_tokens in the request body (OpenAI format)
                # Note: 'original_body' here is the harvested body. We need to check the incoming 'messages' or 'body' from the request.
                # But wait, 'stream_chat' doesn't receive the full request body, only 'messages' and 'model'.
                # Let's assume we want to restore the high limit.
                
                if isinstance(gen_config, dict):
                    # Restore high limit or use a safe default
                    # If the harvested token had a value, we keep it (unless we want to force it)
                    # User requested to put it back to 65535
                    if 'maxOutputTokens' in gen_config:
                        # Ensure it's at least 8192 if it was lowered, or just set to 65535 if missing/low
                        if gen_config['maxOutputTokens'] < 8192:
                                gen_config['maxOutputTokens'] = 65535
                    else:
                        gen_config['maxOutputTokens'] = 65535
                
                if 'temperature' in kwargs and kwargs['temperature'] is not None:
                    gen_config['temperature'] = float(kwargs['temperature'])
                    
                if 'top_p' in kwargs and kwargs['top_p'] is not None:
                    gen_config['topP'] = float(kwargs['top_p'])
                    
                if 'top_k' in kwargs and kwargs['top_k'] is not None:
                    gen_config['topK'] = int(kwargs['top_k'])
                    
                if 'max_tokens' in kwargs and kwargs['max_tokens'] is not None:
                    gen_config['maxOutputTokens'] = int(kwargs['max_tokens'])
                    
                if 'stop' in kwargs and kwargs['stop'] is not None:
                    gen_config['stopSequences'] = kwargs['stop'] if isinstance(kwargs['stop'], list) else [kwargs['stop']]

                # Reassemble body
                new_body = {
                    "querySignature": original_body.get('querySignature'), # Might need this?
                    "operationName": original_body.get('operationName'),
                    "variables": new_variables
                }
                
                # 3. Prepare Headers
                headers = creds['headers'].copy() # Copy to avoid mutating the cached credentials
                
                # Ensure critical headers are present and correct
                # Note: 'Cookie', 'User-Agent', 'Origin', 'Referer' should now be in creds['headers'] from the harvester
                
                headers['content-type'] = 'application/json'
                
                # Remove headers that httpx/network layer should handle or that might cause conflicts
                headers.pop('content-length', None)
                headers.pop('Content-Length', None)
                headers.pop('host', None)
                headers.pop('Host', None)
                headers.pop('connection', None)
                headers.pop('Connection', None)
                headers.pop('accept-encoding', None) # Let httpx handle decompression

                url = creds['url']
                
                # ç®€åŒ–æ—¥å¿— - ä»…åœ¨é¦–æ¬¡è¯·æ±‚æ—¶æ‰“å°æ¨¡å‹å
                if attempt == 0:
                    print(f"â†’ {backend_model_for_api}")
                else:
                    print(f"â†» é‡è¯•({attempt+1})")
                try:
                    # ä½¿ç”¨ç‹¬ç«‹å®¢æˆ·ç«¯è¿›è¡Œæµå¼è¯·æ±‚,ç¡®ä¿è¯·æ±‚é—´å®Œå…¨éš”ç¦»
                    async with isolated_client.stream('POST', url, headers=headers, json=new_body) as response:
                        print(f"ğŸ“¡ Response Status: {response.status_code}")
                    
                        if response.status_code != 200:
                            error_text = await response.aread()
                            print(f"âœ— API é”™è¯¯: {response.status_code}")
                            
                            # Check for potential token expiration
                            if response.status_code in [400, 401, 403] and attempt < max_retries:
                                print(f"[{request_id}] âš ï¸ è®¤è¯é”™è¯¯ ({response.status_code})ï¼Œè§¦å‘åˆ·æ–°...")
                                
                                # Trigger UI Refresh
                                if self.request_token_refresh:
                                    await self.request_token_refresh()
                                
                                # ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶ç­‰å¾…æ–°å‡­è¯ï¼ˆæ›´å¿«å“åº”ï¼‰
                                refresh_start = time.time()
                                refreshed = await self.cred_manager.wait_for_credential_with_queue(request_id, timeout=30)
                                refresh_elapsed = time.time() - refresh_start
                                
                                if refreshed:
                                    # éªŒè¯å‡­è¯ç‰ˆæœ¬æ˜¯å¦æ›´æ–°
                                    new_version = self.cred_manager.credential_version
                                    if new_version > current_cred_version:
                                        print(f"[{request_id}] âœ… å‡­è¯å·²æ›´æ–° v{current_cred_version} â†’ v{new_version} ({refresh_elapsed:.1f}ç§’)")
                                        
                                        await asyncio.sleep(0.3)  # çŸ­æš‚å»¶è¿Ÿ
                                        # Update headers/url with new credentials
                                        new_creds = self.cred_manager.get_credentials()
                                        headers = new_creds['headers'].copy()
                                        headers['content-type'] = 'application/json'
                                        headers.pop('content-length', None)
                                        headers.pop('host', None)
                                        url = new_creds['url']
                                        print(f"[{request_id}] ğŸ”„ ä½¿ç”¨æ–°å‡­è¯é‡è¯•...")
                                        continue # Retry loop
                                    else:
                                        print(f"[{request_id}] âš ï¸ å‡­è¯ç‰ˆæœ¬æœªå˜åŒ–")
                                else:
                                    print(f"[{request_id}] âš ï¸ å‡­è¯åˆ·æ–°è¶…æ—¶ ({refresh_elapsed:.1f}ç§’)")
                            
                            # If we get here, it's a fatal error or retry failed
                            error_payload = {"error": {"message": f"Upstream Error: {response.status_code} - {error_text.decode()}", "type": "upstream_error"}}
                            yield f"data: {json.dumps(error_payload)}\n\n"
                            return

                        # Layer 1: ä½¿ç”¨ChunkAggregatorç¨³å®šè¾“å…¥æµ
                        # v5.0: å¢åŠ min_chunk_sizeä»¥ç¡®ä¿JSONè¾¹ç•Œç¨³å®šæ€§
                        aggregator = ChunkAggregator(min_chunk_size=256, max_buffer_time=0.1)
                        stabilized_stream = aggregator.aggregate(response.aiter_text())
                        
                        # ä½¿ç”¨StreamProcessorå¤„ç†å“åº”æµ
                        chunk_count = 0
                        total_completion_chars = 0
                        stream_error = None  # v8.1: è¿½è¸ªæµå¤„ç†ä¸­çš„é”™è¯¯
                        
                        try:
                            async for sse_event in stream_processor.process_stream(stabilized_stream, model=model):
                                chunk_count += 1
                                # ç»Ÿè®¡completionå­—ç¬¦æ•°ç”¨äºtokenä¼°ç®—
                                if 'data: ' in sse_event and '"content"' in sse_event:
                                    try:
                                        json_part = sse_event.split('data: ', 1)[1].split('\n')[0]
                                        if json_part != '[DONE]':
                                            chunk_obj = json.loads(json_part)
                                            delta_content = chunk_obj.get('choices', [{}])[0].get('delta', {}).get('content', '')
                                            if delta_content:
                                                total_completion_chars += len(delta_content)
                                    except:
                                        pass
                                yield sse_event
                                # v8.3: ä½¿ç”¨ stream_processor è¿½è¸ªå®é™…å†…å®¹æ˜¯å¦å·²å‘é€
                                # role chunk å’Œ heartbeat chunk ä¸ç®—å®é™…å†…å®¹ï¼Œä»å¯é‡è¯•
                                content_yielded = stream_processor.has_actual_content_sent()
                                await asyncio.sleep(0)
                        except (AuthError, StreamAuthError) as e:
                            # v8.1: æ•è·æµå¤„ç†ä¸­çš„è®¤è¯é”™è¯¯
                            stream_error = e
                            print(f"âš ï¸ æµä¸­æ£€æµ‹åˆ°è®¤è¯é”™è¯¯")
                        
                        # v8.1: å¦‚æœæµå¤„ç†ä¸­å‘ç”Ÿè®¤è¯é”™è¯¯ï¼Œè§¦å‘é‡è¯•
                        if stream_error:
                            if content_yielded:
                                # å·²å‘é€å†…å®¹ï¼Œæ— æ³•é‡è¯•
                                print("âš ï¸ å·²å‘é€å†…å®¹ï¼Œæ— æ³•é‡è¯•")
                                error_payload = {"error": {"message": f"Authentication failed mid-stream: {str(stream_error)}", "type": "authentication_error"}}
                                yield f"data: {json.dumps(error_payload)}\n\n"
                                yield "data: [DONE]\n\n"
                                return
                            
                            if attempt < max_retries:
                                print(f"[{request_id}] ğŸ”„ æµä¸­è®¤è¯é”™è¯¯ï¼Œè§¦å‘åˆ·æ–° (å°è¯• {attempt+1}/{max_retries+1})")
                                
                                # è§¦å‘åˆ·æ–°
                                if self.request_token_refresh:
                                    await self.request_token_refresh()
                                
                                # ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶ç­‰å¾…æ–°å‡­è¯
                                refresh_start = time.time()
                                refreshed = await self.cred_manager.wait_for_credential_with_queue(request_id, timeout=30)
                                refresh_elapsed = time.time() - refresh_start
                                
                                if refreshed:
                                    # éªŒè¯å‡­è¯ç‰ˆæœ¬æ˜¯å¦æ›´æ–°
                                    new_version = self.cred_manager.credential_version
                                    if new_version > current_cred_version:
                                        print(f"[{request_id}] âœ… å‡­è¯å·²æ›´æ–° v{current_cred_version} â†’ v{new_version} ({refresh_elapsed:.1f}ç§’)")
                                        await asyncio.sleep(0.3)
                                        print(f"[{request_id}] ğŸ”„ ä½¿ç”¨æ–°å‡­è¯é‡è¯•...")
                                        continue  # é‡è¯•å¾ªç¯
                                    else:
                                        print(f"[{request_id}] âš ï¸ å‡­è¯ç‰ˆæœ¬æœªå˜åŒ–")
                                else:
                                    print(f"[{request_id}] âš ï¸ å‡­è¯åˆ·æ–°è¶…æ—¶ ({refresh_elapsed:.1f}ç§’)")
                            
                            # é‡è¯•ç”¨å°½æˆ–åˆ·æ–°å¤±è´¥ - é™é»˜å¤±è´¥ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨å¤„ç†
                            print(f"âš ï¸ å‡­è¯åˆ·æ–°å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                            # ä¸å‘å®¢æˆ·ç«¯è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œè®©è¯·æ±‚é™é»˜å¤±è´¥
                            return
                        
                        # ä¼°ç®—å¹¶æ›´æ–°tokenç»Ÿè®¡
                        # å›¾åƒæ¨¡å‹ä½¿ç”¨å›ºå®štokenè®¡æ•°ï¼ŒLLMä½¿ç”¨å­—ç¬¦ä¼°ç®—
                        is_image_model = "image" in backend_model_for_api.lower()
                        
                        if is_image_model:
                            # å›¾åƒæ¨¡å‹: ä½¿ç”¨å›ºå®šçš„ä¼°ç®—å€¼
                            # è¾“å…¥çº¦500 tokenï¼Œè¾“å‡ºå›¾åƒçº¦1000 token
                            prompt_tokens = 500
                            completion_tokens = 1000
                        else:
                            # LLM: æ ¹æ®å®é™…å†…å®¹ä¼°ç®—
                            prompt_tokens = self.stats_manager.estimate_messages_tokens(messages)
                            completion_tokens = max(1, int(total_completion_chars / 3.5)) if total_completion_chars > 0 else 1
                        
                        await self.stats_manager.update(prompt_tokens, completion_tokens, model=model)
                        self.stats_manager.set_current_request_tokens(prompt_tokens, completion_tokens)
                        
                        # å‘é€åŒ…å«usageçš„æœ€ç»ˆchunkç»™å®¢æˆ·ç«¯
                        usage_chunk = {
                            "id": f"chatcmpl-proxy-usage-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": None}],
                            "usage": {
                                "prompt_tokens": prompt_tokens,
                                "completion_tokens": completion_tokens,
                                "total_tokens": prompt_tokens + completion_tokens
                            }
                        }
                        yield f"data: {json.dumps(usage_chunk)}\n\n"
                        
                        # v8.1: åªæœ‰æˆåŠŸå®Œæˆæ‰å‘é€[DONE]
                        yield "data: [DONE]\n\n"
                        
                        # ç®€åŒ–å®Œæˆæ—¥å¿—
                        if is_image_model:
                            print(f"âœ… å›¾åƒç”Ÿæˆå®Œæˆ")
                        else:
                            print(f"âœ… {chunk_count} å— | {prompt_tokens}+{completion_tokens}={prompt_tokens+completion_tokens} token")
                        
                        # å¦‚æœæˆåŠŸå¤„ç†å®Œæµï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break

                except (AuthError, StreamAuthError) as e:
                    print(f"âš ï¸ è®¤è¯é”™è¯¯")
                    
                    # å¦‚æœå·²ç»å‘é€äº†å†…å®¹ï¼Œä¸èƒ½é‡è¯•
                    if content_yielded:
                        print("âš ï¸ å·²å‘é€å†…å®¹ï¼Œæ— æ³•é‡è¯•")
                        error_payload = {"error": {"message": f"Authentication failed mid-stream: {str(e)}", "type": "authentication_error"}}
                        yield f"data: {json.dumps(error_payload)}\n\n"
                        return

                    if attempt < max_retries:
                        print("ğŸ”„ è§¦å‘åˆ·æ–°å¹¶é‡è¯•...")
                        if self.request_token_refresh:
                            await self.request_token_refresh()
                        # Step 1: Wait for the new credentials to be harvested
                        refreshed = await self.cred_manager.wait_for_refresh(timeout=60)
                        if refreshed:
                            ui_ready = await self.cred_manager.wait_for_refresh_complete(timeout=60)
                            if ui_ready:
                                print("âœ… å‡­è¯å’Œ UI å·²å°±ç»ª")
                                await asyncio.sleep(1) # Add 1 second delay
                                # Update headers/url with new credentials
                                new_creds = self.cred_manager.get_credentials()
                                headers = new_creds['headers'].copy()
                                headers['content-type'] = 'application/json'
                                headers.pop('content-length', None)
                                headers.pop('host', None)
                                url = new_creds['url']
                                continue # Retry the request
                            else:
                                print("âœ— UI æœªå°±ç»ª")
                        else:
                            print("âœ— åˆ·æ–°è¶…æ—¶")

                    error_payload = {"error": {"message": str(e), "type": "authentication_error"}}
                    yield f"data: {json.dumps(error_payload)}\n\n"
                    return

                except Exception as e:
                    print(f"âœ— è¯·æ±‚å¤±è´¥: {str(e)[:50]}")
                    
                    if content_yielded:
                        print("âš ï¸ å·²å‘é€å†…å®¹ï¼Œæ— æ³•é‡è¯•")
                        error_payload = {"error": {"message": f"Stream interrupted: {str(e)}", "type": "request_error"}}
                        yield f"data: {json.dumps(error_payload)}\n\n"
                        return

                    if attempt < max_retries:
                        continue
                    error_payload = {"error": {"message": str(e), "type": "request_error"}}
                    yield f"data: {json.dumps(error_payload)}\n\n"
                    return # Stop generator on fatal error
        
        finally:
            # ç¡®ä¿ç‹¬ç«‹å®¢æˆ·ç«¯è¢«æ­£ç¡®å…³é—­,é‡Šæ”¾èµ„æº
            await isolated_client.aclose()