"""FastAPIè·¯ç”±æ¨¡å—"""

import asyncio
import json
import os
import time
import uuid
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, Response, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from typing import Dict, Any, List

from src.core import MODELS_CONFIG_FILE, TokenStatsManager, load_config
from src.api.vertex_client import VertexAIClient


class APIKeyAuthMiddleware(BaseHTTPMiddleware):
    """API å¯†é’¥éªŒè¯ä¸­é—´ä»¶"""
    
    def __init__(self, app, api_keys: List[str]):
        super().__init__(app)
        self.api_keys = api_keys
        self.enabled = len(api_keys) > 0
    
    async def dispatch(self, request: Request, call_next):
        # å¦‚æœæœªé…ç½®å¯†é’¥ï¼Œåˆ™ä¸éªŒè¯
        if not self.enabled:
            return await call_next(request)
        
        # è·³è¿‡å¥åº·æ£€æŸ¥ç«¯ç‚¹å’Œç»Ÿè®¡é¡µé¢
        if request.url.path in ["/health", "/", "/v1/models", "/stats", "/api/stats"]:
            return await call_next(request)
        
        # ä» Authorization å¤´è·å–å¯†é’¥
        auth_header = request.headers.get("Authorization", "")
        
        # æ”¯æŒ "Bearer sk-xxx" å’Œ "sk-xxx" ä¸¤ç§æ ¼å¼
        if auth_header.startswith("Bearer "):
            api_key = auth_header[7:]
        else:
            api_key = auth_header
        
        # éªŒè¯å¯†é’¥
        if api_key not in self.api_keys:
            return Response(
                content=json.dumps({
                    "error": {
                        "message": "Invalid API key",
                        "type": "invalid_request_error",
                        "code": "invalid_api_key"
                    }
                }),
                status_code=401,
                media_type="application/json"
            )
        
        return await call_next(request)


class ConnectionCompatibilityMiddleware(BaseHTTPMiddleware):
    """
    è¿æ¥å…¼å®¹æ€§ä¸­é—´ä»¶
    
    è§£å†³ httpx ç­‰ç°ä»£ HTTP å®¢æˆ·ç«¯çš„è¿æ¥é—®é¢˜ï¼š
    - ç¡®ä¿æ­£ç¡®çš„ Connection å¤´å¤„ç†
    - æ”¯æŒ HTTP/1.0 å’Œ HTTP/1.1 å®¢æˆ·ç«¯
    """
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # ç¡®ä¿å“åº”åŒ…å«é€‚å½“çš„è¿æ¥å¤´
        # æŸäº›å®¢æˆ·ç«¯ï¼ˆå¦‚ httpxï¼‰éœ€è¦æ˜ç¡®çš„ keep-alive æ”¯æŒ
        if "connection" not in response.headers:
            response.headers["Connection"] = "keep-alive"
        
        return response


def create_app(vertex_client: VertexAIClient, stats_manager: TokenStatsManager) -> FastAPI:
    """åˆ›å»ºFastAPIåº”ç”¨"""
    app = FastAPI()
    
    # ä»ç¯å¢ƒå˜é‡è¯»å– API å¯†é’¥ï¼ˆé€—å·åˆ†éš”ï¼‰
    api_keys_env = os.getenv("API_KEYS", "")
    if api_keys_env:
        api_keys = [key.strip() for key in api_keys_env.split(",") if key.strip()]
        print(f"ğŸ” API å¯†é’¥éªŒè¯å·²å¯ç”¨ ({len(api_keys)} ä¸ªå¯†é’¥)")
    else:
        api_keys = []
        print("âš ï¸ API å¯†é’¥éªŒè¯æœªå¯ç”¨ï¼ˆæœªè®¾ç½® API_KEYS ç¯å¢ƒå˜é‡ï¼‰")
    
    app.add_middleware(APIKeyAuthMiddleware, api_keys=api_keys)
    
    # æ·»åŠ è¿æ¥å…¼å®¹æ€§ä¸­é—´ä»¶
    app.add_middleware(ConnectionCompatibilityMiddleware)
    
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Allows all origins
        allow_credentials=True,
        allow_methods=["*"],  # Allows all methods
        allow_headers=["*"],  # Allows all headers
        expose_headers=["*"],  # æš´éœ²æ‰€æœ‰å“åº”å¤´ç»™å®¢æˆ·ç«¯
    )
    
    @app.get("/")
    async def root():
        """æ ¹è·¯å¾„é‡å®šå‘åˆ°ç»Ÿè®¡é¡µé¢"""
        from fastapi.responses import RedirectResponse
        return RedirectResponse(url="/stats")
    
    @app.get("/v1/models")
    async def list_models():
        """è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        current_time = int(time.time())
        models = []
        try:
            with open(MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                models = config.get('models', [])
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ models.json å¤±è´¥: {e}")
            models = ["gemini-1.5-pro", "gemini-1.5-flash"]

        data = {
            "object": "list",
            "data": [
                {"id": m, "object": "model", "created": current_time, "owned_by": "google"}
                for m in models
            ]
        }
        return data

    @app.post("/v1/chat/completions")
    async def chat_completions(request: Request):
        """å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚"""
        try:
            body = await request.json()
            messages = body.get('messages', [])
            model = body.get('model', 'gemini-1.5-pro')
            stream = body.get('stream', False)
            
            temperature = body.get('temperature')
            top_p = body.get('top_p')
            top_k = body.get('top_k')
            max_tokens = body.get('max_tokens')
            stop = body.get('stop')
            tools = body.get('tools')
            
            if not messages:
                if stream:
                    async def empty_stream_generator():
                        empty_chunk = {
                            "id": f"chatcmpl-proxy-empty-{uuid.uuid4()}",
                            "object": "chat.completion.chunk",
                            "created": int(time.time()),
                            "model": model,
                            "choices": [{"index": 0, "delta": {"role": "assistant", "content": ""}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(empty_chunk)}\n\n"
                        yield "data: [DONE]\n\n"
                    return StreamingResponse(empty_stream_generator(), media_type="text/event-stream")
                else:
                    return {
                        "id": f"chatcmpl-proxy-empty-{uuid.uuid4()}",
                        "object": "chat.completion",
                        "created": int(time.time()),
                        "model": model,
                        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                        "choices": [{
                            "index": 0,
                            "message": {"role": "assistant", "content": ""},
                            "finish_reason": "stop"
                        }]
                    }

            if stream:
                async def stream_with_disconnect_check():
                    """åŒ…è£…æµå¼å“åº”ï¼Œæ·»åŠ å®¢æˆ·ç«¯æ–­å¼€æ£€æµ‹"""
                    try:
                        async for chunk in vertex_client.stream_chat(
                            messages,
                            model,
                            temperature=temperature,
                            top_p=top_p,
                            top_k=top_k,
                            max_tokens=max_tokens,
                            stop=stop,
                            tools=tools
                        ):
                            if await request.is_disconnected():
                                print("âš ï¸ å®¢æˆ·ç«¯æ–­å¼€ï¼Œç»ˆæ­¢å“åº”")
                                break
                            yield chunk
                    except asyncio.CancelledError:
                        print("âš ï¸ å“åº”å·²å–æ¶ˆ")
                        raise
                
                # å¢å¼ºçš„ SSE å“åº”å¤´ï¼Œæå‡ httpx ç­‰å®¢æˆ·ç«¯å…¼å®¹æ€§
                sse_headers = {
                    "Cache-Control": "no-cache, no-store, must-revalidate, max-age=0",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
                    "Transfer-Encoding": "chunked",
                }
                
                return StreamingResponse(
                    stream_with_disconnect_check(),
                    media_type="text/event-stream",
                    headers=sse_headers
                )
            else:
                response_data = await vertex_client.complete_chat(
                    messages,
                    model,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_tokens=max_tokens,
                    stop=stop,
                    tools=tools
                )
                return response_data

        except HTTPException:
            raise
        except Exception as e:
            print(f"âš ï¸ ç«¯ç‚¹å¼‚å¸¸: {e}")
            raise HTTPException(status_code=500, detail={"error": str(e)})
    
    @app.get("/api/stats")
    async def get_stats():
        """è·å–æ¯æ—¥ç»Ÿè®¡æ•°æ®"""
        try:
            daily_stats = stats_manager.get_daily_stats()
            return {
                "success": True,
                "data": daily_stats
            }
        except Exception as e:
            print(f"âš ï¸ è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail={"error": str(e)})
    
    @app.get("/stats")
    async def stats_page():
        """ç»Ÿè®¡é¡µé¢"""
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        current_file = os.path.abspath(__file__)
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))
        stats_html_path = os.path.join(project_root, "static", "stats.html")
        
        print(f"ğŸ” æŸ¥æ‰¾ç»Ÿè®¡é¡µé¢: {stats_html_path}")
        print(f"   æ–‡ä»¶å­˜åœ¨: {os.path.exists(stats_html_path)}")
        
        if os.path.exists(stats_html_path):
            return FileResponse(stats_html_path, media_type="text/html")
        else:
            return Response(
                content=f"ç»Ÿè®¡é¡µé¢æœªæ‰¾åˆ°ã€‚æŸ¥æ‰¾è·¯å¾„: {stats_html_path}",
                status_code=404,
                media_type="text/plain"
            )
    
    return app