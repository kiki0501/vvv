"""
æµå¼å“åº”å¤„ç†å™¨

å¤„ç† Vertex AI æµå¼å“åº”å¹¶è½¬æ¢ä¸º OpenAI SSE æ ¼å¼ã€‚
æ”¯æŒå†…å®¹å»é‡ã€Diffå—åŸå­ä¼ è¾“ã€å¿ƒè·³ä¿æ´»ã€‚
"""

import json
import time
import uuid
from typing import Dict, Any, Generator, Optional, List, AsyncGenerator
from threading import Lock

from .trackers import DiffState, PathIndexTracker, StreamBuffer
from .parsers import IncrementalJSONParser
from .diff_handler import DiffBlockHandler
from .sse_formatter import SSEFormatter


class AuthError(Exception):
    """è®¤è¯é”™è¯¯"""
    pass


class StreamProcessor:
    """æµå¼å“åº”å¤„ç†å™¨ï¼Œæ”¯æŒä¸‰å±‚æ¶ˆæŠ–å’Œå†…å®¹å»é‡"""
    
    TAIL_BUFFER_SIZE = 512  # å°¾éƒ¨ç¼“å†²åŒºå¤§å°ï¼Œç”¨äºå¾®é‡å¤è£å‰ª
    
    def __init__(self, enable_heartbeat: bool = True, heartbeat_interval: float = 15.0):
        """
        åˆå§‹åŒ–æµå¤„ç†å™¨
        
        Args:
            enable_heartbeat: æ˜¯å¦å¯ç”¨å¿ƒè·³æœºåˆ¶
            heartbeat_interval: å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰
        """
        self.enable_heartbeat = enable_heartbeat
        self.heartbeat_interval = heartbeat_interval
        self.debug_mode = False
        self._conversation_id = str(uuid.uuid4())
        self._lock = Lock()
        
        self.json_parser = IncrementalJSONParser()
        self.diff_handler = DiffBlockHandler()
        self.buffer = StreamBuffer()
        self.path_tracker = PathIndexTracker()
        self.sse_formatter = SSEFormatter(self._conversation_id)
        
        self._tail_buffer = ""
        self._tail_buffer_lock = Lock()
        self._role_sent = False
        self._actual_content_sent = False  # ç”¨äºåˆ¤æ–­æ˜¯å¦å¯å®‰å…¨é‡è¯•
        
        self._stats = {
            "chunks_processed": 0,
            "chunks_yielded": 0,
            "duplicates_filtered": 0,
            "diff_blocks_processed": 0,
            "prefix_trimmed_bytes": 0,
            "errors": 0
        }
        
        self._last_chunk_time = time.time()
        self._chunk_times = []
        self._chunk_sizes = []
        
    def enable_debug(self, enabled: bool = True):
        """å¯ç”¨è°ƒè¯•æ¨¡å¼"""
        self.debug_mode = enabled
    
    def _log_debug(self, message: str):
        """è°ƒè¯•æ—¥å¿—"""
        if self.debug_mode:
            print(f"[æµå¤„ç†] {message}")
    
    def _generate_chunk_id(self, sequence: int) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„chunk ID"""
        return f"{self._conversation_id[:8]}-seq{sequence:06d}"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self._stats,
            "buffer_stats": self.buffer.get_stats(),
            "tracker_stats": self.path_tracker.get_stats(),
            "parser_stats": self.json_parser.get_stats()
        }
    
    def has_actual_content_sent(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å‘é€å®é™…æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºé‡è¯•åˆ¤æ–­ï¼‰"""
        return self._actual_content_sent

    def _trim_duplicate_prefix(self, content: str) -> str:
        """è£å‰ªä¸å°¾éƒ¨ç¼“å†²åŒºé‡å çš„å‰ç¼€"""
        with self._tail_buffer_lock:
            if not self._tail_buffer or not content:
                return content
            
            # æŸ¥æ‰¾contentå‰ç¼€ä¸tail_bufferåç¼€çš„æœ€å¤§é‡å 
            max_overlap = min(len(self._tail_buffer), len(content))
            overlap_len = 0
            
            for i in range(1, max_overlap + 1):
                # æ£€æŸ¥ tail_buffer çš„å i ä¸ªå­—ç¬¦æ˜¯å¦ç­‰äº content çš„å‰ i ä¸ªå­—ç¬¦
                if self._tail_buffer[-i:] == content[:i]:
                    overlap_len = i
            
            if overlap_len > 0:
                self._stats["prefix_trimmed_bytes"] += overlap_len
                trimmed = content[overlap_len:]
                if self.debug_mode:
                    print(f"ğŸ”§ è£å‰ªé‡å¤å‰ç¼€: {overlap_len}å­—ç¬¦")
                return trimmed
            
            return content
    
    def _update_tail_buffer(self, content: str):
        """æ›´æ–°å°¾éƒ¨ç¼“å†²åŒº"""
        with self._tail_buffer_lock:
            self._tail_buffer += content
            # ä¿æŒç¼“å†²åŒºä¸è¶…è¿‡æœ€å¤§å¤§å°
            if len(self._tail_buffer) > self.TAIL_BUFFER_SIZE:
                self._tail_buffer = self._tail_buffer[-self.TAIL_BUFFER_SIZE:]

    def _yield_content(
        self,
        content: str,
        model: str,
        is_diff_block: bool = False,
        is_reasoning: bool = False
    ) -> Generator[str, None, None]:
        """è¾“å‡ºå†…å®¹å—ï¼ˆå¸¦é‡å¤å‰ç¼€è£å‰ªï¼‰"""
        if not content:
            return
        
        if not self._role_sent:
            self._role_sent = True
            yield self.sse_formatter.create_initial_role_chunk(model)
            self.buffer.mark_yield()
        
        trimmed_content = self._trim_duplicate_prefix(content)
        if not trimmed_content:
            self._stats["duplicates_filtered"] += 1
            return
        
        sequence = self.buffer.increment_sequence()
        chunk_id = self._generate_chunk_id(sequence)
        
        if is_reasoning:
            openai_chunk = self.sse_formatter.create_openai_chunk(reasoning_content=trimmed_content, model=model)
        else:
            openai_chunk = self.sse_formatter.create_openai_chunk(content=trimmed_content, model=model)
        
        sse_event = self.sse_formatter.format_sse_event(data=openai_chunk)
        
        self._update_tail_buffer(trimmed_content)
        
        self.buffer.mark_yield()
        self.buffer.mark_content_sent(trimmed_content)
        self._stats["chunks_yielded"] += 1
        self._actual_content_sent = True
        
        if is_diff_block:
            self._stats["diff_blocks_processed"] += 1
        
        yield sse_event

    def _fix_base64_padding(self, b64_data: str) -> str:
        """
        ä¿®å¤ base64 å¡«å……
        
        base64 ç¼–ç çš„æ•°æ®é•¿åº¦å¿…é¡»æ˜¯ 4 çš„å€æ•°ï¼Œä¸è¶³æ—¶éœ€è¦ç”¨ '=' å¡«å……ã€‚
        å¦‚æœæ•°æ®è¢«æˆªæ–­æˆ–ç¼ºå°‘å¡«å……ï¼Œä¼šå¯¼è‡´è§£ç é”™è¯¯å’Œå›¾åƒæŸåã€‚
        """
        if not b64_data:
            return b64_data
        
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
        b64_data = b64_data.replace('\n', '').replace('\r', '').replace(' ', '')
        
        # è®¡ç®—éœ€è¦çš„å¡«å……
        missing_padding = len(b64_data) % 4
        if missing_padding:
            b64_data += '=' * (4 - missing_padding)
        
        return b64_data

    def _yield_content_raw(
        self,
        content: str,
        model: str
    ) -> Generator[str, None, None]:
        """
        è¾“å‡ºåŸå§‹å†…å®¹å—ï¼ˆä¸åšé‡å¤å‰ç¼€è£å‰ªï¼‰
        
        ç”¨äºå›¾åƒç­‰äºŒè¿›åˆ¶æ•°æ®ï¼Œé¿å… base64 è¢«é”™è¯¯è£å‰ªå¯¼è‡´å›¾åƒæŸå
        """
        if not content:
            return
        
        if not self._role_sent:
            self._role_sent = True
            yield self.sse_formatter.create_initial_role_chunk(model)
            self.buffer.mark_yield()
        
        sequence = self.buffer.increment_sequence()
        
        openai_chunk = self.sse_formatter.create_openai_chunk(content=content, model=model)
        sse_event = self.sse_formatter.format_sse_event(data=openai_chunk)
        
        # å›¾åƒæ•°æ®ä¸æ›´æ–° tail_bufferï¼Œé¿å…å½±å“åç»­æ–‡æœ¬çš„å»é‡
        # self._update_tail_buffer(content)  # è·³è¿‡
        
        self.buffer.mark_yield()
        self.buffer.mark_content_sent(content)
        self._stats["chunks_yielded"] += 1
        self._actual_content_sent = True
        
        yield sse_event

    def _extract_path_index(self, result: Dict[str, Any]) -> int:
        """ä»resultä¸­æå–pathç´¢å¼•ï¼Œæ ¼å¼: {"path": [..., ç´¢å¼•]}"""
        path = result.get('path', [])
        if len(path) >= 3:
            try:
                return int(path[2])
            except (ValueError, TypeError):
                return -1
        return -1

    def process_vertex_response(
        self,
        data: Dict[str, Any],
        model: str = "vertex-ai-proxy"
    ) -> Generator[str, None, None]:
        """å¤„ç†Vertex AIå“åº”å¹¶è½¬æ¢ä¸ºOpenAI SSEæ ¼å¼"""
        self._stats["chunks_processed"] += 1
        
        if not data:
            return
        
        # æ£€æŸ¥é”™è¯¯
        if 'error' in data:
            self._log_debug(f"Vertex AIé”™è¯¯: {data['error']}")
            self._stats["errors"] += 1
            return
        
        results = data.get('results', [])
        if not results:
            return
        
        indexed_results = []
        for result in results:
            if not result:
                continue
            path_index = self._extract_path_index(result)
            indexed_results.append((path_index, result))
        
        # æŒ‰pathç´¢å¼•æ’åºï¼ˆ-1ä¼šæ’åœ¨æœ€å‰é¢ï¼Œè¿™äº›æ˜¯æ²¡æœ‰pathçš„resultï¼‰
        indexed_results.sort(key=lambda x: x[0] if x[0] >= 0 else float('inf'))
        
        for path_index, result in indexed_results:
            # æ£€æŸ¥é”™è¯¯
            if 'errors' in result:
                for err in result['errors']:
                    msg = err.get('message', 'Unknown Error')
                    self._log_debug(f"APIé”™è¯¯: {msg}")
                    self._stats["errors"] += 1
                    
                    # æŠ›å‡ºè®¤è¯é”™è¯¯ä»¥ä¾¿ä¸Šå±‚é‡è¯•
                    if "Recaptcha" in msg or "token" in msg.lower() or "Authentication" in msg:
                        raise AuthError(f"Authentication failed: {msg}")
                continue
            
            result_data = result.get('data')
            if not result_data:
                continue
            
            candidates = result_data.get('candidates')
            if not candidates:
                continue
            
            for candidate in candidates:
                content_obj = candidate.get('content') or {}
                parts = content_obj.get('parts') or []
                
                # ä¸¥æ ¼æŒ‰é¡ºåºå¤„ç† parts
                for part in parts:
                    # 1. å¤„ç†æ–‡æœ¬å†…å®¹ (åŒ…æ‹¬ thought)
                    text = part.get('text', '')
                    if text:
                        is_thought = part.get('thought', False)
                        
                        if path_index >= 0:
                            tracker_result = self.path_tracker.process_result(path_index, text, is_thought)
                            
                            if tracker_result:
                                _, delta_content, is_reasoning = tracker_result
                                if delta_content:
                                    yield from self._yield_content(delta_content, model, is_reasoning=is_reasoning)
                            else:
                                self._stats["duplicates_filtered"] += 1
                        else:
                            if is_thought:
                                yield from self._yield_content(text, model, is_reasoning=True)
                            else:
                                yield from self._yield_content(text, model, is_diff_block=False)
                    
                    inline_data = part.get('inlineData')
                    uri = part.get('uri')
                    
                    if inline_data:
                        mime_type = inline_data.get('mimeType')
                        b64_data = inline_data.get('data')
                        if mime_type and b64_data:
                            # éªŒè¯å’Œä¿®å¤ base64 æ•°æ®
                            b64_data = self._fix_base64_padding(b64_data)
                            image_md = f"![Generated Image](data:{mime_type};base64,{b64_data})"
                            # å›¾åƒæ•°æ®ä¸åšé‡å¤å‰ç¼€è£å‰ªï¼Œç›´æ¥è¾“å‡º
                            yield from self._yield_content_raw(image_md, model)
                    elif uri:
                        image_md = f"![Generated Image]({uri})"
                        yield from self._yield_content_raw(image_md, model)
                
                finish_reason = candidate.get('finishReason')
                if finish_reason in ['STOP', 'MAX_TOKENS']:
                    if not self._role_sent:
                        self._role_sent = True
                        yield self.sse_formatter.create_initial_role_chunk(model)
                        self.buffer.mark_yield()
                    
                    sequence = self.buffer.increment_sequence()
                    chunk_id = self._generate_chunk_id(sequence)
                    
                    mapped_reason = self.sse_formatter.map_finish_reason(finish_reason)
                    
                    finish_chunk = self.sse_formatter.create_openai_chunk(
                        finish_reason=mapped_reason,
                        model=model
                    )
                    
                    sse_event = self.sse_formatter.format_sse_event(data=finish_chunk)
                    
                    yield sse_event
                    self.buffer.mark_yield()
    
    async def process_stream(
        self,
        response_iterator,
        model: str = "vertex-ai-proxy"
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†å®Œæ•´çš„æµå¼å“åº”
        
        Args:
            response_iterator: å“åº”æ•°æ®çš„å¼‚æ­¥è¿­ä»£å™¨
            model: æ¨¡å‹åç§°
        
        Yields:
            SSEæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        content_yielded = False
        is_error = False
        
        try:
            chunk_received = 0
            async for chunk in response_iterator:
                chunk_received += 1
                current_time = time.time()
                
                self._chunk_times.append(current_time)
                self._chunk_sizes.append(len(chunk))
                if len(self._chunk_times) > 20:
                    self._chunk_times.pop(0)
                    self._chunk_sizes.pop(0)
                
                self._last_chunk_time = current_time
                
                if chunk_received <= 3:
                    self._log_debug(f"æ”¶åˆ° chunk #{chunk_received}: {len(chunk)} å­—ç¬¦")
                
                json_objects = self.json_parser.feed(chunk)
                
                if chunk_received <= 3:
                    self._log_debug(f"  è§£æå‡º {len(json_objects)} ä¸ª JSON å¯¹è±¡")
                
                if len(chunk) > 500 and len(chunk) < 10000 and len(json_objects) == 0 and chunk_received > 1:
                    self._log_debug(f"âš ï¸ å¤§ chunk ä½†æ—  JSON: {len(chunk)} å­—ç¬¦")
                
                for obj in json_objects:
                    for sse_event in self.process_vertex_response(obj, model):
                        yield sse_event
                        content_yielded = True
                
                if self.enable_heartbeat and self.buffer.should_send_heartbeat(self.heartbeat_interval):
                    sequence = self.buffer.increment_sequence()
                    heartbeat = self.sse_formatter.create_heartbeat_event(sequence)
                    yield heartbeat
                    self.buffer.mark_yield()
                    
        except Exception as e:
            self._log_debug(f"æµå¤„ç†å¼‚å¸¸: {str(e)[:100]}")
            is_error = True
            raise e
            
        finally:
            if is_error:
                self._log_debug(f"æµå¤„ç†å®Œæˆ (æœ‰é”™è¯¯)")
            else:
                remaining_json_objs = self.json_parser.flush()
                for obj in remaining_json_objs:
                    for sse_event in self.process_vertex_response(obj, model):
                        yield sse_event
                        content_yielded = True
                
                diff_flush_result = self.diff_handler.flush()
                if diff_flush_result:
                    flush_content, is_diff = diff_flush_result
                    if flush_content:
                        self._log_debug(f"DiffHandler flush: {len(flush_content)} å­—ç¬¦")
                        for sse_event in self._yield_content(flush_content, model, is_diff_block=is_diff):
                            yield sse_event
                            content_yielded = True
                
                pending_contents = self.path_tracker.get_pending_content()
                for path_idx, pending_content, is_thought in pending_contents:
                    if pending_content:
                        self._log_debug(f"PathTracker flush: {len(pending_content)} å­—ç¬¦")
                        for sse_event in self._yield_content(pending_content, model, is_reasoning=is_thought):
                            yield sse_event
                            content_yielded = True
                
                if not content_yielded:
                    self._log_debug("âš ï¸ æ— å†…å®¹è¾“å‡ºï¼Œå‘é€ç©ºæ¶ˆæ¯")
                    if not self._role_sent:
                        self._role_sent = True
                        yield self.sse_formatter.create_initial_role_chunk(model)
                        self.buffer.mark_yield()
                    empty_chunk = self.sse_formatter.create_openai_chunk(content="", model=model)
                    yield self.sse_formatter.format_sse_event(data=empty_chunk)
                    finish_chunk = self.sse_formatter.create_openai_chunk(finish_reason="stop", model=model)
                    yield self.sse_formatter.format_sse_event(data=finish_chunk)
                    self.buffer.mark_yield()
                
                yield "data: [DONE]\n\n"


def get_stream_processor(enable_heartbeat: bool = True, heartbeat_interval: float = 15.0) -> StreamProcessor:
    """åˆ›å»ºæµå¤„ç†å™¨å®ä¾‹"""
    return StreamProcessor(enable_heartbeat=enable_heartbeat, heartbeat_interval=heartbeat_interval)